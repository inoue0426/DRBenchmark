{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e0edc6-9ffb-4fef-9803-6b3cf999548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from load_data import load_data\n",
    "from sampler import NewSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from DeepDSC.DeepDSC import (AE, DF, GeneExpressionDataset,\n",
    "                             calculate_morgan_fingerprints, prepare_data,\n",
    "                             prepare_drug_data, prepare_train_val_test_data,\n",
    "                             train_autoencoder, train_df_model)\n",
    "\n",
    "data = \"nci\"\n",
    "PATH = \"../nci_data/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a90b41a-ba20-418e-a775-e64b6cc8ae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load nci\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.device = device  # cuda:number or cpu\n",
    "        self.data = \"nci\"  # Dataset{gdsc or ccle}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "res, drug_feature, exprs, mut, cna, null_mask, pos_num = load_data(args)\n",
    "cells = {i: j for i, j in enumerate(res.index)}\n",
    "drugs = {i: j for i, j in enumerate(res.columns)}\n",
    "\n",
    "cell_sum = np.sum(res.values, axis=1)\n",
    "drug_sum = np.sum(res.values, axis=0)\n",
    "\n",
    "target_dim = [\n",
    "    # 0,  # Drug\n",
    "    1  # Cell\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d654467-d3f5-4fac-ad4a-0ee550f6fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(PATH, train, test):\n",
    "    print(\"Loading gene expression data...\")\n",
    "    normalized_gene_exp_tensor, gene_exp = prepare_data(\n",
    "        data1=PATH + \"/gene_exp_part1.csv.gz\", data2=PATH + \"gene_exp_part2.csv.gz\"\n",
    "    )\n",
    "    normalized_gene_exp_dataset = GeneExpressionDataset(normalized_gene_exp_tensor)\n",
    "    normalized_gene_exp_dataloader = DataLoader(\n",
    "        normalized_gene_exp_dataset, batch_size=10000, shuffle=True\n",
    "    )\n",
    "\n",
    "    # オートエンコーダーのトレーニング\n",
    "    print(\"Training autoencoder...\")\n",
    "    autoencoder = AE(normalized_gene_exp_tensor.shape[1]).to(device)\n",
    "    train_autoencoder(autoencoder, normalized_gene_exp_dataloader)\n",
    "    print(\"Autoencoder training completed.\")\n",
    "\n",
    "    # 圧縮特徴の抽出\n",
    "    print(\"Extracting compressed features...\")\n",
    "    compressed_features_tensor = autoencoder.encoder(normalized_gene_exp_tensor)\n",
    "    compressed_features = pd.DataFrame(\n",
    "        compressed_features_tensor.cpu().detach().numpy(), index=gene_exp.columns\n",
    "    )\n",
    "    print(f\"Compressed features shape: {compressed_features.shape}\")\n",
    "    drug_response, nsc_sm = prepare_drug_data(is_nsc=True)\n",
    "    mfp = calculate_morgan_fingerprints(drug_response.T, nsc_sm)\n",
    "    print(f\"Morgan fingerprints shape: {mfp.shape}\")\n",
    "\n",
    "    train_labels = train[2]\n",
    "    val_labels = test[2]\n",
    "    train_data = train[[1, 0]]\n",
    "    train_data.columns = [0, 1]\n",
    "    val_data = test[[1, 0]]\n",
    "    val_data.columns = [0, 1]\n",
    "    print(\n",
    "        f\"Training data size: {len(train_data)}, Validation data size: {len(val_data)}\"\n",
    "    )\n",
    "    train_data, val_data = prepare_train_val_test_data(\n",
    "        train_data, val_data, compressed_features, mfp\n",
    "    )\n",
    "    df_model = DF().to(device)\n",
    "    val_labels, best_val_out = train_df_model(\n",
    "        df_model,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        torch.tensor(train_labels).double().to(device),\n",
    "        torch.tensor(val_labels).double().to(device),\n",
    "    )\n",
    "    print(\"DF model training completed.\")\n",
    "    return val_labels, best_val_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded5b5ee-4019-4a87-a833-b5423b547189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepDSC(res_mat, null_mask, target_dim, target_index, seed):\n",
    "    sampler = NewSampler(res_mat, null_mask, target_dim, target_index, seed)\n",
    "\n",
    "    train_data = pd.DataFrame(sampler.train_data, index=res.index, columns=res.columns)\n",
    "    test_data = pd.DataFrame(sampler.test_data, index=res.index, columns=res.columns)\n",
    "\n",
    "    train_mask = pd.DataFrame(sampler.train_mask, index=res.index, columns=res.columns)\n",
    "    test_mask = pd.DataFrame(sampler.test_mask, index=res.index, columns=res.columns)\n",
    "\n",
    "    train = pd.DataFrame(train_mask.values.nonzero()).T\n",
    "    train[2] = train_data.values[train_mask.values.nonzero()].astype(int)\n",
    "\n",
    "    test = pd.DataFrame(test_mask.values.nonzero()).T\n",
    "    test[2] = test_data.values[test_mask.values.nonzero()].astype(int)\n",
    "\n",
    "    val_labels = test[2]\n",
    "\n",
    "    if len(np.unique(val_labels)) < 2:\n",
    "        print(f\"Target {target_index} skipped: Validation set has only one class.\")\n",
    "        return None, None\n",
    "\n",
    "    train[0] = [cells[i] for i in train[0]]\n",
    "    train[1] = [drugs[i] for i in train[1]]\n",
    "    test[0] = [cells[i] for i in test[0]]\n",
    "    test[1] = [drugs[i] for i in test[1]]\n",
    "\n",
    "    val_labels, best_val_out = main(PATH, train, test)\n",
    "    return val_labels, best_val_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06207807-b1b3-43b3-af0b-98d338222993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1005 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gene expression data...\n",
      "Training autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/800 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/800 [00:00<09:59,  1.33it/s]\u001b[A\n",
      "  2%|▏         | 15/800 [00:00<00:33, 23.43it/s]\u001b[A\n",
      "  3%|▎         | 24/800 [00:00<00:23, 33.05it/s]\u001b[A\n",
      "  4%|▍         | 32/800 [00:01<00:18, 40.54it/s]\u001b[A\n",
      "  5%|▍         | 39/800 [00:01<00:16, 46.32it/s]\u001b[A\n",
      "  6%|▌         | 46/800 [00:01<00:14, 51.19it/s]\u001b[A\n",
      "  7%|▋         | 53/800 [00:01<00:13, 55.12it/s]\u001b[A\n",
      "  8%|▊         | 60/800 [00:01<00:12, 58.23it/s]\u001b[A\n",
      "  8%|▊         | 67/800 [00:01<00:12, 60.57it/s]\u001b[A\n",
      "  9%|▉         | 74/800 [00:01<00:11, 62.31it/s]\u001b[A\n",
      " 10%|█         | 81/800 [00:01<00:11, 63.58it/s]\u001b[A\n",
      " 11%|█         | 88/800 [00:01<00:11, 64.53it/s]\u001b[A\n",
      " 12%|█▏        | 95/800 [00:02<00:10, 65.18it/s]\u001b[A\n",
      " 13%|█▎        | 102/800 [00:02<00:10, 65.60it/s]\u001b[A\n",
      " 14%|█▎        | 109/800 [00:02<00:10, 65.90it/s]\u001b[A\n",
      " 14%|█▍        | 116/800 [00:02<00:10, 66.14it/s]\u001b[A\n",
      " 15%|█▌        | 123/800 [00:02<00:10, 66.27it/s]\u001b[A\n",
      " 16%|█▋        | 130/800 [00:02<00:10, 66.38it/s]\u001b[A\n",
      " 17%|█▋        | 137/800 [00:02<00:09, 66.48it/s]\u001b[A\n",
      " 18%|█▊        | 144/800 [00:02<00:09, 66.56it/s]\u001b[A\n",
      " 19%|█▉        | 151/800 [00:02<00:09, 66.56it/s]\u001b[A\n",
      " 20%|█▉        | 158/800 [00:03<00:09, 66.57it/s]\u001b[A\n",
      " 21%|██        | 165/800 [00:03<00:09, 66.62it/s]\u001b[A\n",
      " 22%|██▏       | 172/800 [00:03<00:09, 66.63it/s]\u001b[A\n",
      " 22%|██▏       | 179/800 [00:03<00:09, 66.64it/s]\u001b[A\n",
      " 23%|██▎       | 186/800 [00:03<00:09, 66.65it/s]\u001b[A\n",
      " 24%|██▍       | 193/800 [00:03<00:09, 66.63it/s]\u001b[A\n",
      " 25%|██▌       | 200/800 [00:03<00:09, 66.63it/s]\u001b[A\n",
      " 26%|██▌       | 207/800 [00:03<00:08, 66.63it/s]\u001b[A\n",
      " 27%|██▋       | 214/800 [00:03<00:08, 66.64it/s]\u001b[A\n",
      " 28%|██▊       | 221/800 [00:03<00:08, 66.63it/s]\u001b[A\n",
      " 28%|██▊       | 228/800 [00:04<00:08, 66.63it/s]\u001b[A\n",
      " 29%|██▉       | 235/800 [00:04<00:08, 66.65it/s]\u001b[A\n",
      " 30%|███       | 242/800 [00:04<00:08, 66.65it/s]\u001b[A\n",
      " 31%|███       | 249/800 [00:04<00:08, 66.66it/s]\u001b[A\n",
      " 32%|███▏      | 256/800 [00:04<00:08, 66.66it/s]\u001b[A\n",
      " 33%|███▎      | 263/800 [00:04<00:08, 66.64it/s]\u001b[A\n",
      " 34%|███▍      | 270/800 [00:04<00:07, 66.64it/s]\u001b[A\n",
      " 35%|███▍      | 277/800 [00:04<00:07, 66.65it/s]\u001b[A\n",
      " 36%|███▌      | 284/800 [00:04<00:07, 66.63it/s]\u001b[A\n",
      " 36%|███▋      | 291/800 [00:05<00:07, 66.63it/s]\u001b[A\n",
      " 37%|███▋      | 298/800 [00:05<00:07, 66.65it/s]\u001b[A\n",
      " 38%|███▊      | 305/800 [00:05<00:07, 66.63it/s]\u001b[A\n",
      " 39%|███▉      | 312/800 [00:05<00:07, 66.66it/s]\u001b[A\n",
      " 40%|███▉      | 319/800 [00:05<00:07, 66.64it/s]\u001b[A\n",
      " 41%|████      | 326/800 [00:05<00:07, 66.64it/s]\u001b[A\n",
      " 42%|████▏     | 333/800 [00:05<00:07, 66.63it/s]\u001b[A\n",
      " 42%|████▎     | 340/800 [00:05<00:06, 66.63it/s]\u001b[A\n",
      " 43%|████▎     | 347/800 [00:05<00:06, 66.61it/s]\u001b[A\n",
      " 44%|████▍     | 354/800 [00:05<00:06, 66.62it/s]\u001b[A\n",
      " 45%|████▌     | 361/800 [00:06<00:06, 66.64it/s]\u001b[A\n",
      " 46%|████▌     | 368/800 [00:06<00:06, 66.65it/s]\u001b[A\n",
      " 47%|████▋     | 375/800 [00:06<00:06, 66.66it/s]\u001b[A\n",
      " 48%|████▊     | 382/800 [00:06<00:06, 66.66it/s]\u001b[A\n",
      " 49%|████▊     | 389/800 [00:06<00:06, 66.65it/s]\u001b[A\n",
      " 50%|████▉     | 396/800 [00:06<00:06, 66.64it/s]\u001b[A\n",
      " 50%|█████     | 403/800 [00:06<00:05, 66.64it/s]\u001b[A\n",
      " 51%|█████▏    | 410/800 [00:06<00:05, 66.68it/s]\u001b[A\n",
      " 52%|█████▏    | 417/800 [00:06<00:05, 66.67it/s]\u001b[A\n",
      " 53%|█████▎    | 424/800 [00:06<00:05, 66.68it/s]\u001b[A\n",
      " 54%|█████▍    | 431/800 [00:07<00:05, 66.64it/s]\u001b[A\n",
      " 55%|█████▍    | 438/800 [00:07<00:05, 66.64it/s]\u001b[A\n",
      " 56%|█████▌    | 445/800 [00:07<00:05, 66.61it/s]\u001b[A\n",
      " 56%|█████▋    | 452/800 [00:07<00:05, 66.64it/s]\u001b[A\n",
      " 57%|█████▋    | 459/800 [00:07<00:05, 66.65it/s]\u001b[A\n",
      " 58%|█████▊    | 466/800 [00:07<00:05, 66.67it/s]\u001b[A\n",
      " 59%|█████▉    | 473/800 [00:07<00:04, 66.66it/s]\u001b[A\n",
      " 60%|██████    | 480/800 [00:07<00:04, 66.69it/s]\u001b[A\n",
      " 61%|██████    | 487/800 [00:07<00:04, 66.66it/s]\u001b[A\n",
      " 62%|██████▏   | 494/800 [00:08<00:04, 66.65it/s]\u001b[A\n",
      " 63%|██████▎   | 501/800 [00:08<00:04, 66.65it/s]\u001b[A\n",
      " 64%|██████▎   | 508/800 [00:08<00:04, 66.63it/s]\u001b[A\n",
      " 64%|██████▍   | 515/800 [00:08<00:04, 66.62it/s]\u001b[A\n",
      " 65%|██████▌   | 522/800 [00:08<00:04, 66.66it/s]\u001b[A\n",
      " 66%|██████▌   | 529/800 [00:08<00:04, 66.69it/s]\u001b[A\n",
      " 67%|██████▋   | 536/800 [00:08<00:03, 66.67it/s]\u001b[A\n",
      " 68%|██████▊   | 543/800 [00:08<00:03, 66.67it/s]\u001b[A\n",
      " 69%|██████▉   | 550/800 [00:08<00:03, 66.69it/s]\u001b[A\n",
      " 70%|██████▉   | 557/800 [00:08<00:03, 66.69it/s]\u001b[A\n",
      " 70%|███████   | 564/800 [00:09<00:03, 66.67it/s]\u001b[A\n",
      " 71%|███████▏  | 571/800 [00:09<00:03, 66.66it/s]\u001b[A\n",
      " 72%|███████▏  | 578/800 [00:09<00:03, 66.68it/s]\u001b[A\n",
      " 73%|███████▎  | 585/800 [00:09<00:03, 66.68it/s]\u001b[A\n",
      " 74%|███████▍  | 592/800 [00:09<00:03, 66.67it/s]\u001b[A\n",
      " 75%|███████▍  | 599/800 [00:09<00:03, 66.65it/s]\u001b[A\n",
      " 76%|███████▌  | 606/800 [00:09<00:02, 66.65it/s]\u001b[A\n",
      " 77%|███████▋  | 613/800 [00:09<00:02, 66.64it/s]\u001b[A\n",
      " 78%|███████▊  | 620/800 [00:09<00:02, 66.65it/s]\u001b[A\n",
      " 78%|███████▊  | 627/800 [00:10<00:02, 66.68it/s]\u001b[A\n",
      " 79%|███████▉  | 634/800 [00:10<00:02, 66.66it/s]\u001b[A\n",
      " 80%|████████  | 641/800 [00:10<00:02, 66.69it/s]\u001b[A\n",
      " 81%|████████  | 648/800 [00:10<00:02, 66.66it/s]\u001b[A\n",
      " 82%|████████▏ | 655/800 [00:10<00:02, 66.65it/s]\u001b[A\n",
      " 83%|████████▎ | 662/800 [00:10<00:02, 66.62it/s]\u001b[A\n",
      " 84%|████████▎ | 669/800 [00:10<00:01, 66.63it/s]\u001b[A\n",
      " 84%|████████▍ | 676/800 [00:10<00:01, 66.66it/s]\u001b[A\n",
      " 85%|████████▌ | 683/800 [00:10<00:01, 66.64it/s]\u001b[A\n",
      " 86%|████████▋ | 690/800 [00:10<00:01, 66.65it/s]\u001b[A\n",
      " 87%|████████▋ | 697/800 [00:11<00:01, 66.67it/s]\u001b[A\n",
      " 88%|████████▊ | 704/800 [00:11<00:01, 66.66it/s]\u001b[A\n",
      " 89%|████████▉ | 711/800 [00:11<00:01, 66.63it/s]\u001b[A\n",
      " 90%|████████▉ | 718/800 [00:11<00:01, 66.62it/s]\u001b[A\n",
      " 91%|█████████ | 725/800 [00:11<00:01, 66.62it/s]\u001b[A\n",
      " 92%|█████████▏| 732/800 [00:11<00:01, 66.61it/s]\u001b[A\n",
      " 92%|█████████▏| 739/800 [00:11<00:00, 66.62it/s]\u001b[A\n",
      " 93%|█████████▎| 746/800 [00:11<00:00, 66.65it/s]\u001b[A\n",
      " 94%|█████████▍| 753/800 [00:11<00:00, 66.66it/s]\u001b[A\n",
      " 95%|█████████▌| 760/800 [00:12<00:00, 66.66it/s]\u001b[A\n",
      " 96%|█████████▌| 767/800 [00:12<00:00, 66.66it/s]\u001b[A\n",
      " 97%|█████████▋| 774/800 [00:12<00:00, 66.69it/s]\u001b[A\n",
      " 98%|█████████▊| 781/800 [00:12<00:00, 66.68it/s]\u001b[A\n",
      " 98%|█████████▊| 788/800 [00:12<00:00, 66.66it/s]\u001b[A\n",
      "100%|██████████| 800/800 [00:12<00:00, 63.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder training completed.\n",
      "Extracting compressed features...\n",
      "Compressed features shape: (60, 500)\n",
      "Morgan fingerprints shape: (1005, 256)\n",
      "Training data size: 57959, Validation data size: 60\n",
      "Epoch 1 Loss: 0.774 Val Loss: 3.281\n",
      "Accuracy: 0.367\n",
      "Epoch 2 Loss: 2.000 Val Loss: 2.348\n",
      "Accuracy: 0.367\n",
      "Epoch 3 Loss: 1.393 Val Loss: 1.076\n",
      "Accuracy: 0.367\n",
      "Epoch 4 Loss: 0.738 Val Loss: 0.657\n",
      "Accuracy: 0.633\n",
      "Epoch 5 Loss: 0.839 Val Loss: 0.687\n",
      "Accuracy: 0.633\n",
      "Epoch 6 Loss: 0.969 Val Loss: 0.666\n",
      "Accuracy: 0.633\n",
      "Epoch 7 Loss: 0.889 Val Loss: 0.669\n",
      "Accuracy: 0.633\n",
      "Epoch 8 Loss: 0.748 Val Loss: 0.777\n",
      "Accuracy: 0.367\n",
      "Epoch 9 Loss: 0.687 Val Loss: 0.938\n",
      "Accuracy: 0.367\n",
      "Epoch 10 Loss: 0.719 Val Loss: 1.048\n",
      "Accuracy: 0.367\n",
      "Epoch 11 Loss: 0.767 Val Loss: 1.061\n",
      "Accuracy: 0.367\n",
      "Epoch 12 Loss: 0.779 Val Loss: 0.995\n",
      "Accuracy: 0.367\n",
      "Epoch 13 Loss: 0.755 Val Loss: 0.894\n",
      "Accuracy: 0.367\n",
      "Epoch 14 Loss: 0.718 Val Loss: 0.798\n",
      "Accuracy: 0.367\n",
      "Epoch 15 Loss: 0.690 Val Loss: 0.728\n",
      "Accuracy: 0.367\n",
      "Epoch 16 Loss: 0.681 Val Loss: 0.688\n",
      "Accuracy: 0.633\n",
      "Epoch 17 Loss: 0.687 Val Loss: 0.670\n",
      "Accuracy: 0.633\n",
      "Epoch 18 Loss: 0.698 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 19 Loss: 0.705 Val Loss: 0.661\n",
      "Accuracy: 0.633\n",
      "Epoch 20 Loss: 0.706 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 21 Loss: 0.701 Val Loss: 0.669\n",
      "Accuracy: 0.633\n",
      "Epoch 22 Loss: 0.693 Val Loss: 0.679\n",
      "Accuracy: 0.633\n",
      "Epoch 23 Loss: 0.685 Val Loss: 0.693\n",
      "Accuracy: 0.367\n",
      "Epoch 24 Loss: 0.679 Val Loss: 0.711\n",
      "Accuracy: 0.367\n",
      "Epoch 25 Loss: 0.678 Val Loss: 0.729\n",
      "Accuracy: 0.367\n",
      "Epoch 26 Loss: 0.679 Val Loss: 0.743\n",
      "Accuracy: 0.367\n",
      "Epoch 27 Loss: 0.682 Val Loss: 0.751\n",
      "Accuracy: 0.367\n",
      "Epoch 28 Loss: 0.684 Val Loss: 0.751\n",
      "Accuracy: 0.367\n",
      "Epoch 29 Loss: 0.684 Val Loss: 0.745\n",
      "Accuracy: 0.367\n",
      "Epoch 30 Loss: 0.683 Val Loss: 0.733\n",
      "Accuracy: 0.367\n",
      "Epoch 31 Loss: 0.680 Val Loss: 0.719\n",
      "Accuracy: 0.367\n",
      "Epoch 32 Loss: 0.677 Val Loss: 0.705\n",
      "Accuracy: 0.367\n",
      "Epoch 33 Loss: 0.675 Val Loss: 0.692\n",
      "Accuracy: 0.633\n",
      "Epoch 34 Loss: 0.673 Val Loss: 0.682\n",
      "Accuracy: 0.633\n",
      "Epoch 35 Loss: 0.673 Val Loss: 0.675\n",
      "Accuracy: 0.633\n",
      "Epoch 36 Loss: 0.673 Val Loss: 0.670\n",
      "Accuracy: 0.633\n",
      "Epoch 37 Loss: 0.674 Val Loss: 0.667\n",
      "Accuracy: 0.633\n",
      "Epoch 38 Loss: 0.674 Val Loss: 0.665\n",
      "Accuracy: 0.633\n",
      "Epoch 39 Loss: 0.674 Val Loss: 0.664\n",
      "Accuracy: 0.633\n",
      "Epoch 40 Loss: 0.674 Val Loss: 0.664\n",
      "Accuracy: 0.633\n",
      "Epoch 41 Loss: 0.673 Val Loss: 0.665\n",
      "Accuracy: 0.633\n",
      "Epoch 42 Loss: 0.672 Val Loss: 0.667\n",
      "Accuracy: 0.633\n",
      "Epoch 43 Loss: 0.671 Val Loss: 0.669\n",
      "Accuracy: 0.633\n",
      "Epoch 44 Loss: 0.670 Val Loss: 0.671\n",
      "Accuracy: 0.633\n",
      "Epoch 45 Loss: 0.670 Val Loss: 0.673\n",
      "Accuracy: 0.633\n",
      "Epoch 46 Loss: 0.670 Val Loss: 0.674\n",
      "Accuracy: 0.633\n",
      "Epoch 47 Loss: 0.670 Val Loss: 0.675\n",
      "Accuracy: 0.633\n",
      "Epoch 48 Loss: 0.670 Val Loss: 0.676\n",
      "Accuracy: 0.633\n",
      "Epoch 49 Loss: 0.670 Val Loss: 0.675\n",
      "Accuracy: 0.633\n",
      "Epoch 50 Loss: 0.669 Val Loss: 0.674\n",
      "Accuracy: 0.633\n",
      "Epoch 51 Loss: 0.669 Val Loss: 0.673\n",
      "Accuracy: 0.633\n",
      "Epoch 52 Loss: 0.669 Val Loss: 0.671\n",
      "Accuracy: 0.633\n",
      "Epoch 53 Loss: 0.668 Val Loss: 0.669\n",
      "Accuracy: 0.633\n",
      "Epoch 54 Loss: 0.668 Val Loss: 0.667\n",
      "Accuracy: 0.633\n",
      "Epoch 55 Loss: 0.667 Val Loss: 0.665\n",
      "Accuracy: 0.633\n",
      "Epoch 56 Loss: 0.667 Val Loss: 0.664\n",
      "Accuracy: 0.633\n",
      "Epoch 57 Loss: 0.667 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 58 Loss: 0.667 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 59 Loss: 0.667 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 60 Loss: 0.667 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 61 Loss: 0.666 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 62 Loss: 0.666 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 63 Loss: 0.666 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 64 Loss: 0.666 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 65 Loss: 0.665 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 66 Loss: 0.665 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 67 Loss: 0.665 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 68 Loss: 0.665 Val Loss: 0.664\n",
      "Accuracy: 0.633\n",
      "Epoch 69 Loss: 0.665 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 70 Loss: 0.665 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 71 Loss: 0.665 Val Loss: 0.663\n",
      "Accuracy: 0.633\n",
      "Epoch 72 Loss: 0.664 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 73 Loss: 0.664 Val Loss: 0.662\n",
      "Accuracy: 0.633\n",
      "Epoch 74 Loss: 0.664 Val Loss: 0.661\n",
      "Accuracy: 0.633\n",
      "Epoch 75 Loss: 0.664 Val Loss: 0.661\n",
      "Accuracy: 0.633\n",
      "Epoch 76 Loss: 0.664 Val Loss: 0.660\n",
      "Accuracy: 0.633\n",
      "Epoch 77 Loss: 0.664 Val Loss: 0.660\n",
      "Accuracy: 0.633\n",
      "Epoch 78 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 79 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 80 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 81 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 82 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 83 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 84 Loss: 0.663 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 85 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 86 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 87 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 88 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 89 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 90 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 91 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 92 Loss: 0.662 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 93 Loss: 0.661 Val Loss: 0.659\n",
      "Accuracy: 0.633\n",
      "Epoch 94 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n",
      "Epoch 95 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1005 [00:24<6:54:16, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n",
      "Epoch 97 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n",
      "Epoch 98 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n",
      "Epoch 99 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n",
      "Epoch 100 Loss: 0.661 Val Loss: 0.658\n",
      "Accuracy: 0.633\n",
      "DF model training completed.\n",
      "Loading gene expression data...\n",
      "Training autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/800 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 14/800 [00:00<00:05, 136.65it/s]\u001b[A\n",
      "  4%|▎         | 28/800 [00:00<00:09, 82.92it/s] \u001b[A\n",
      "  5%|▍         | 38/800 [00:00<00:10, 75.98it/s]\u001b[A\n",
      "  6%|▌         | 47/800 [00:00<00:10, 72.67it/s]\u001b[A\n",
      "  7%|▋         | 55/800 [00:00<00:10, 70.81it/s]\u001b[A\n",
      "  8%|▊         | 63/800 [00:00<00:10, 69.54it/s]\u001b[A\n",
      "  9%|▉         | 71/800 [00:00<00:10, 68.69it/s]\u001b[A\n",
      " 10%|▉         | 78/800 [00:01<00:10, 68.14it/s]\u001b[A\n",
      " 11%|█         | 85/800 [00:01<00:10, 67.74it/s]\u001b[A\n",
      " 12%|█▏        | 92/800 [00:01<00:10, 67.44it/s]\u001b[A\n",
      " 12%|█▏        | 99/800 [00:01<00:10, 67.22it/s]\u001b[A\n",
      " 13%|█▎        | 106/800 [00:01<00:10, 67.09it/s]\u001b[A\n",
      " 14%|█▍        | 113/800 [00:01<00:10, 66.99it/s]\u001b[A\n",
      " 15%|█▌        | 120/800 [00:01<00:10, 66.92it/s]\u001b[A\n",
      " 16%|█▌        | 127/800 [00:01<00:10, 66.84it/s]\u001b[A\n",
      " 17%|█▋        | 134/800 [00:01<00:09, 66.79it/s]\u001b[A\n",
      " 18%|█▊        | 141/800 [00:02<00:09, 66.77it/s]\u001b[A\n",
      " 18%|█▊        | 148/800 [00:02<00:09, 66.79it/s]\u001b[A\n",
      " 19%|█▉        | 155/800 [00:02<00:09, 66.78it/s]\u001b[A\n",
      " 20%|██        | 162/800 [00:02<00:09, 66.71it/s]\u001b[A\n",
      " 21%|██        | 169/800 [00:02<00:09, 66.71it/s]\u001b[A\n",
      " 22%|██▏       | 176/800 [00:02<00:09, 66.70it/s]\u001b[A\n",
      " 23%|██▎       | 183/800 [00:02<00:09, 66.68it/s]\u001b[A\n",
      " 24%|██▍       | 190/800 [00:02<00:09, 66.67it/s]\u001b[A\n",
      " 25%|██▍       | 197/800 [00:02<00:09, 66.65it/s]\u001b[A\n",
      " 26%|██▌       | 204/800 [00:02<00:08, 66.64it/s]\u001b[A\n",
      " 26%|██▋       | 211/800 [00:03<00:08, 66.67it/s]\u001b[A\n",
      " 27%|██▋       | 218/800 [00:03<00:08, 66.66it/s]\u001b[A\n",
      " 28%|██▊       | 225/800 [00:03<00:08, 66.70it/s]\u001b[A\n",
      " 29%|██▉       | 232/800 [00:03<00:08, 66.71it/s]\u001b[A\n",
      " 30%|██▉       | 239/800 [00:03<00:08, 66.72it/s]\u001b[A\n",
      " 31%|███       | 246/800 [00:03<00:08, 66.73it/s]\u001b[A\n",
      " 32%|███▏      | 253/800 [00:03<00:08, 66.73it/s]\u001b[A\n",
      " 32%|███▎      | 260/800 [00:03<00:08, 66.72it/s]\u001b[A\n",
      " 33%|███▎      | 267/800 [00:03<00:07, 66.71it/s]\u001b[A\n",
      " 34%|███▍      | 274/800 [00:04<00:07, 66.68it/s]\u001b[A\n",
      " 35%|███▌      | 281/800 [00:04<00:07, 66.69it/s]\u001b[A\n",
      " 36%|███▌      | 288/800 [00:04<00:07, 66.66it/s]\u001b[A\n",
      " 37%|███▋      | 295/800 [00:04<00:07, 66.65it/s]\u001b[A\n",
      " 38%|███▊      | 302/800 [00:04<00:07, 66.67it/s]\u001b[A\n",
      " 39%|███▊      | 309/800 [00:04<00:07, 66.70it/s]\u001b[A\n",
      " 40%|███▉      | 316/800 [00:04<00:07, 66.65it/s]\u001b[A\n",
      " 40%|████      | 323/800 [00:04<00:07, 66.66it/s]\u001b[A\n",
      " 41%|████▏     | 330/800 [00:04<00:07, 66.68it/s]\u001b[A\n",
      " 42%|████▏     | 337/800 [00:04<00:06, 66.67it/s]\u001b[A\n",
      " 43%|████▎     | 344/800 [00:05<00:06, 66.69it/s]\u001b[A\n",
      " 44%|████▍     | 351/800 [00:05<00:06, 66.68it/s]\u001b[A\n",
      " 45%|████▍     | 358/800 [00:05<00:06, 66.66it/s]\u001b[A\n",
      " 46%|████▌     | 365/800 [00:05<00:06, 66.68it/s]\u001b[A\n",
      " 46%|████▋     | 372/800 [00:05<00:06, 66.67it/s]\u001b[A\n",
      " 47%|████▋     | 379/800 [00:05<00:06, 66.67it/s]\u001b[A\n",
      " 48%|████▊     | 386/800 [00:05<00:06, 66.69it/s]\u001b[A\n",
      " 49%|████▉     | 393/800 [00:05<00:06, 66.70it/s]\u001b[A\n",
      " 50%|█████     | 400/800 [00:05<00:05, 66.71it/s]\u001b[A\n",
      " 51%|█████     | 407/800 [00:06<00:05, 66.73it/s]\u001b[A\n",
      " 52%|█████▏    | 414/800 [00:06<00:05, 66.69it/s]\u001b[A\n",
      " 53%|█████▎    | 421/800 [00:06<00:05, 66.71it/s]\u001b[A\n",
      " 54%|█████▎    | 428/800 [00:06<00:05, 66.73it/s]\u001b[A\n",
      " 54%|█████▍    | 435/800 [00:06<00:05, 66.72it/s]\u001b[A\n",
      " 55%|█████▌    | 442/800 [00:06<00:05, 66.74it/s]\u001b[A\n",
      " 56%|█████▌    | 449/800 [00:06<00:05, 66.68it/s]\u001b[A\n",
      " 57%|█████▋    | 456/800 [00:06<00:05, 66.70it/s]\u001b[A\n",
      " 58%|█████▊    | 463/800 [00:06<00:05, 66.70it/s]\u001b[A\n",
      " 59%|█████▉    | 470/800 [00:06<00:04, 66.71it/s]\u001b[A\n",
      " 60%|█████▉    | 477/800 [00:07<00:04, 66.70it/s]\u001b[A\n",
      " 60%|██████    | 484/800 [00:07<00:04, 66.73it/s]\u001b[A\n",
      " 61%|██████▏   | 491/800 [00:07<00:04, 66.75it/s]\u001b[A\n",
      " 62%|██████▏   | 498/800 [00:07<00:04, 66.74it/s]\u001b[A\n",
      " 63%|██████▎   | 505/800 [00:07<00:04, 66.72it/s]\u001b[A\n",
      " 64%|██████▍   | 512/800 [00:07<00:04, 66.72it/s]\u001b[A\n",
      " 65%|██████▍   | 519/800 [00:07<00:04, 66.73it/s]\u001b[A\n",
      " 66%|██████▌   | 526/800 [00:07<00:04, 66.69it/s]\u001b[A\n",
      " 67%|██████▋   | 533/800 [00:07<00:04, 66.69it/s]\u001b[A\n",
      " 68%|██████▊   | 540/800 [00:07<00:03, 66.69it/s]\u001b[A\n",
      " 68%|██████▊   | 547/800 [00:08<00:03, 66.69it/s]\u001b[A\n",
      " 69%|██████▉   | 554/800 [00:08<00:03, 66.70it/s]\u001b[A\n",
      " 70%|███████   | 561/800 [00:08<00:03, 66.71it/s]\u001b[A\n",
      " 71%|███████   | 568/800 [00:08<00:03, 66.71it/s]\u001b[A\n",
      " 72%|███████▏  | 575/800 [00:08<00:03, 66.71it/s]\u001b[A\n",
      " 73%|███████▎  | 582/800 [00:08<00:03, 66.72it/s]\u001b[A\n",
      " 74%|███████▎  | 589/800 [00:08<00:03, 66.69it/s]\u001b[A\n",
      " 74%|███████▍  | 596/800 [00:08<00:03, 66.67it/s]\u001b[A\n",
      " 75%|███████▌  | 603/800 [00:08<00:02, 66.71it/s]\u001b[A\n",
      " 76%|███████▋  | 610/800 [00:09<00:02, 66.69it/s]\u001b[A\n",
      " 77%|███████▋  | 617/800 [00:09<00:02, 66.69it/s]\u001b[A\n",
      " 78%|███████▊  | 624/800 [00:09<00:02, 66.68it/s]\u001b[A\n",
      " 79%|███████▉  | 631/800 [00:09<00:02, 66.71it/s]\u001b[A\n",
      " 80%|███████▉  | 638/800 [00:09<00:02, 66.70it/s]\u001b[A\n",
      " 81%|████████  | 645/800 [00:09<00:02, 66.72it/s]\u001b[A\n",
      " 82%|████████▏ | 652/800 [00:09<00:02, 66.69it/s]\u001b[A\n",
      " 82%|████████▏ | 659/800 [00:09<00:02, 66.71it/s]\u001b[A\n",
      " 83%|████████▎ | 666/800 [00:09<00:02, 66.70it/s]\u001b[A\n",
      " 84%|████████▍ | 673/800 [00:09<00:01, 66.67it/s]\u001b[A\n",
      " 85%|████████▌ | 680/800 [00:10<00:01, 66.66it/s]\u001b[A\n",
      " 86%|████████▌ | 687/800 [00:10<00:01, 66.68it/s]\u001b[A\n",
      " 87%|████████▋ | 694/800 [00:10<00:01, 66.70it/s]\u001b[A\n",
      " 88%|████████▊ | 701/800 [00:10<00:01, 66.69it/s]\u001b[A\n",
      " 88%|████████▊ | 708/800 [00:10<00:01, 66.69it/s]\u001b[A\n",
      " 89%|████████▉ | 715/800 [00:10<00:01, 66.71it/s]\u001b[A\n",
      " 90%|█████████ | 722/800 [00:10<00:01, 66.66it/s]\u001b[A\n",
      " 91%|█████████ | 729/800 [00:10<00:01, 66.69it/s]\u001b[A\n",
      " 92%|█████████▏| 736/800 [00:10<00:00, 66.69it/s]\u001b[A\n",
      " 93%|█████████▎| 743/800 [00:11<00:00, 66.69it/s]\u001b[A\n",
      " 94%|█████████▍| 750/800 [00:11<00:00, 66.69it/s]\u001b[A\n",
      " 95%|█████████▍| 757/800 [00:11<00:00, 66.70it/s]\u001b[A\n",
      " 96%|█████████▌| 764/800 [00:11<00:00, 66.68it/s]\u001b[A\n",
      " 96%|█████████▋| 771/800 [00:11<00:00, 66.67it/s]\u001b[A\n",
      " 97%|█████████▋| 778/800 [00:11<00:00, 66.70it/s]\u001b[A\n",
      " 98%|█████████▊| 785/800 [00:11<00:00, 66.70it/s]\u001b[A\n",
      " 99%|█████████▉| 792/800 [00:11<00:00, 66.68it/s]\u001b[A\n",
      "100%|██████████| 800/800 [00:11<00:00, 67.27it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder training completed.\n",
      "Extracting compressed features...\n",
      "Compressed features shape: (60, 500)\n",
      "Morgan fingerprints shape: (1005, 256)\n",
      "Training data size: 57959, Validation data size: 60\n",
      "Epoch 1 Loss: 0.774 Val Loss: 1.199\n",
      "Accuracy: 0.417\n",
      "Epoch 2 Loss: 2.002 Val Loss: 0.894\n",
      "Accuracy: 0.417\n",
      "Epoch 3 Loss: 1.393 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 4 Loss: 0.738 Val Loss: 0.809\n",
      "Accuracy: 0.583\n",
      "Epoch 5 Loss: 0.839 Val Loss: 0.874\n",
      "Accuracy: 0.583\n",
      "Epoch 6 Loss: 0.969 Val Loss: 0.819\n",
      "Accuracy: 0.583\n",
      "Epoch 7 Loss: 0.889 Val Loss: 0.728\n",
      "Accuracy: 0.583\n",
      "Epoch 8 Loss: 0.748 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 9 Loss: 0.687 Val Loss: 0.693\n",
      "Accuracy: 0.583\n",
      "Epoch 10 Loss: 0.719 Val Loss: 0.724\n",
      "Accuracy: 0.417\n",
      "Epoch 11 Loss: 0.767 Val Loss: 0.738\n",
      "Accuracy: 0.417\n",
      "Epoch 12 Loss: 0.779 Val Loss: 0.728\n",
      "Accuracy: 0.417\n",
      "Epoch 13 Loss: 0.755 Val Loss: 0.707\n",
      "Accuracy: 0.417\n",
      "Epoch 14 Loss: 0.718 Val Loss: 0.689\n",
      "Accuracy: 0.583\n",
      "Epoch 15 Loss: 0.690 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 16 Loss: 0.681 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 17 Loss: 0.687 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 18 Loss: 0.698 Val Loss: 0.687\n",
      "Accuracy: 0.583\n",
      "Epoch 19 Loss: 0.705 Val Loss: 0.688\n",
      "Accuracy: 0.583\n",
      "Epoch 20 Loss: 0.706 Val Loss: 0.686\n",
      "Accuracy: 0.583\n",
      "Epoch 21 Loss: 0.701 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 22 Loss: 0.693 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 23 Loss: 0.685 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 24 Loss: 0.679 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 25 Loss: 0.678 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 26 Loss: 0.679 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 27 Loss: 0.682 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 28 Loss: 0.684 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 29 Loss: 0.684 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 30 Loss: 0.683 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 31 Loss: 0.680 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 32 Loss: 0.677 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 33 Loss: 0.675 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 34 Loss: 0.673 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 35 Loss: 0.673 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 36 Loss: 0.673 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 37 Loss: 0.674 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 38 Loss: 0.674 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 39 Loss: 0.674 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 40 Loss: 0.674 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 41 Loss: 0.673 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 42 Loss: 0.672 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 43 Loss: 0.671 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 44 Loss: 0.670 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 45 Loss: 0.670 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 46 Loss: 0.670 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 47 Loss: 0.670 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 48 Loss: 0.670 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 49 Loss: 0.670 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 50 Loss: 0.669 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 51 Loss: 0.669 Val Loss: 0.679\n",
      "Accuracy: 0.583\n",
      "Epoch 52 Loss: 0.669 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 53 Loss: 0.668 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 54 Loss: 0.668 Val Loss: 0.680\n",
      "Accuracy: 0.583\n",
      "Epoch 55 Loss: 0.667 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 56 Loss: 0.667 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 57 Loss: 0.667 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 58 Loss: 0.667 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 59 Loss: 0.667 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 60 Loss: 0.667 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 61 Loss: 0.666 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 62 Loss: 0.666 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 63 Loss: 0.666 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 64 Loss: 0.666 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 65 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 66 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 67 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 68 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 69 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 70 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 71 Loss: 0.665 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 72 Loss: 0.664 Val Loss: 0.681\n",
      "Accuracy: 0.583\n",
      "Epoch 73 Loss: 0.664 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 74 Loss: 0.664 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 75 Loss: 0.664 Val Loss: 0.682\n",
      "Accuracy: 0.583\n",
      "Epoch 76 Loss: 0.664 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 77 Loss: 0.664 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 78 Loss: 0.663 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 79 Loss: 0.663 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 80 Loss: 0.663 Val Loss: 0.683\n",
      "Accuracy: 0.583\n",
      "Epoch 81 Loss: 0.663 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 82 Loss: 0.663 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 83 Loss: 0.663 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 84 Loss: 0.663 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 85 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 86 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 87 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 88 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 89 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 90 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 91 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 92 Loss: 0.662 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 93 Loss: 0.661 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 94 Loss: 0.661 Val Loss: 0.684\n",
      "Accuracy: 0.583\n",
      "Epoch 95 Loss: 0.661 Val Loss: 0.684\n",
      "Accuracy: 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1005 [00:44<6:04:37, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Loss: 0.661 Val Loss: 0.685\n",
      "Accuracy: 0.583\n",
      "Epoch 97 Loss: 0.661 Val Loss: 0.685\n",
      "Accuracy: 0.583\n",
      "Epoch 98 Loss: 0.661 Val Loss: 0.685\n",
      "Accuracy: 0.583\n",
      "Epoch 99 Loss: 0.661 Val Loss: 0.685\n",
      "Accuracy: 0.583\n",
      "Epoch 100 Loss: 0.661 Val Loss: 0.685\n",
      "Accuracy: 0.583\n",
      "DF model training completed.\n",
      "Loading gene expression data...\n",
      "Training autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/800 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 15/800 [00:00<00:05, 131.71it/s]\u001b[A\n",
      "  4%|▎         | 29/800 [00:00<00:09, 84.04it/s] \u001b[A\n",
      "  5%|▍         | 39/800 [00:00<00:09, 76.70it/s]\u001b[A\n",
      "  6%|▌         | 48/800 [00:00<00:10, 73.15it/s]\u001b[A\n",
      "  7%|▋         | 56/800 [00:00<00:10, 71.15it/s]\u001b[A\n",
      "  8%|▊         | 64/800 [00:00<00:10, 69.76it/s]\u001b[A\n",
      "  9%|▉         | 72/800 [00:00<00:10, 68.86it/s]\u001b[A\n",
      " 10%|▉         | 79/800 [00:01<00:10, 68.26it/s]\u001b[A\n",
      " 11%|█         | 86/800 [00:01<00:10, 67.80it/s]\u001b[A\n",
      " 12%|█▏        | 93/800 [00:01<00:10, 67.48it/s]\u001b[A\n",
      " 12%|█▎        | 100/800 [00:01<00:10, 67.27it/s]\u001b[A\n",
      " 13%|█▎        | 107/800 [00:01<00:10, 67.11it/s]\u001b[A\n",
      " 14%|█▍        | 114/800 [00:01<00:10, 66.98it/s]\u001b[A\n",
      " 15%|█▌        | 121/800 [00:01<00:10, 66.89it/s]\u001b[A\n",
      " 16%|█▌        | 128/800 [00:01<00:10, 66.87it/s]\u001b[A\n",
      " 17%|█▋        | 135/800 [00:01<00:09, 66.83it/s]\u001b[A\n",
      " 18%|█▊        | 142/800 [00:02<00:09, 66.76it/s]\u001b[A\n",
      " 19%|█▊        | 149/800 [00:02<00:09, 66.75it/s]\u001b[A\n",
      " 20%|█▉        | 156/800 [00:02<00:09, 66.73it/s]\u001b[A\n",
      " 20%|██        | 163/800 [00:02<00:09, 66.68it/s]\u001b[A\n",
      " 21%|██▏       | 170/800 [00:02<00:09, 66.68it/s]\u001b[A\n",
      " 22%|██▏       | 177/800 [00:02<00:09, 66.68it/s]\u001b[A\n",
      " 23%|██▎       | 184/800 [00:02<00:09, 66.68it/s]\u001b[A\n",
      " 24%|██▍       | 191/800 [00:02<00:09, 66.67it/s]\u001b[A\n",
      " 25%|██▍       | 198/800 [00:02<00:09, 66.69it/s]\u001b[A\n",
      " 26%|██▌       | 205/800 [00:02<00:08, 66.71it/s]\u001b[A\n",
      " 26%|██▋       | 212/800 [00:03<00:08, 66.64it/s]\u001b[A\n",
      " 27%|██▋       | 219/800 [00:03<00:08, 66.68it/s]\u001b[A\n",
      " 28%|██▊       | 226/800 [00:03<00:08, 66.67it/s]\u001b[A\n",
      " 29%|██▉       | 233/800 [00:03<00:08, 66.68it/s]\u001b[A\n",
      " 30%|███       | 240/800 [00:03<00:08, 66.69it/s]\u001b[A\n",
      " 31%|███       | 247/800 [00:03<00:08, 66.71it/s]\u001b[A\n",
      " 32%|███▏      | 254/800 [00:03<00:08, 66.69it/s]\u001b[A\n",
      " 33%|███▎      | 261/800 [00:03<00:08, 66.68it/s]\u001b[A\n",
      " 34%|███▎      | 268/800 [00:03<00:07, 66.68it/s]\u001b[A\n",
      " 34%|███▍      | 275/800 [00:04<00:07, 66.70it/s]\u001b[A\n",
      " 35%|███▌      | 282/800 [00:04<00:07, 66.70it/s]\u001b[A\n",
      " 36%|███▌      | 289/800 [00:04<00:07, 66.73it/s]\u001b[A\n",
      " 37%|███▋      | 296/800 [00:04<00:07, 66.73it/s]\u001b[A\n",
      " 38%|███▊      | 303/800 [00:04<00:07, 66.74it/s]\u001b[A\n",
      " 39%|███▉      | 310/800 [00:04<00:07, 66.72it/s]\u001b[A\n",
      " 40%|███▉      | 317/800 [00:04<00:07, 66.69it/s]\u001b[A\n",
      " 40%|████      | 324/800 [00:04<00:07, 66.68it/s]\u001b[A\n",
      " 41%|████▏     | 331/800 [00:04<00:07, 66.67it/s]\u001b[A\n",
      " 42%|████▏     | 339/800 [00:04<00:06, 67.98it/s]\u001b[A\n",
      "  0%|          | 2/1005 [00:51<7:11:50, 25.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m epochs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_kfold):\n\u001b[0;32m---> 14\u001b[0m     val_labels, best_val_out \u001b[38;5;241m=\u001b[39m \u001b[43mDeepDSC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnull_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m true_data_s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m     19\u001b[0m     [true_data_s, pd\u001b[38;5;241m.\u001b[39mDataFrame(val_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m predict_data_s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m     22\u001b[0m     [predict_data_s, pd\u001b[38;5;241m.\u001b[39mDataFrame(best_val_out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mDeepDSC\u001b[0;34m(res_mat, null_mask, target_dim, target_index, seed)\u001b[0m\n\u001b[1;32m     24\u001b[0m test[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m [cells[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     25\u001b[0m test[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m [drugs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 27\u001b[0m val_labels, best_val_out \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_labels, best_val_out\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(PATH, train, test)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining autoencoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m AE(normalized_gene_exp_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_gene_exp_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoencoder training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 圧縮特徴の抽出\u001b[39;00m\n",
      "File \u001b[0;32m/spin1/home/linux/inouey2/DRBenchmark/DeepDSC/DeepDSC/DeepDSC.py:224\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(autoencoder, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m    222\u001b[0m l1_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m autoencoder\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    223\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m+\u001b[39m l1_lambda \u001b[38;5;241m*\u001b[39m l1_norm\n\u001b[0;32m--> 224\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(autoencoder\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    226\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/data/inouey2/conda/envs/genex/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/inouey2/conda/envs/genex/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/inouey2/conda/envs/genex/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_kfold = 1\n",
    "true_data_s = pd.DataFrame()\n",
    "predict_data_s = pd.DataFrame()\n",
    "for dim in target_dim:\n",
    "    for seed, target_index in enumerate(tqdm(np.arange(res.shape[dim]))):\n",
    "        if dim:\n",
    "            if drug_sum[target_index] < 10:\n",
    "                continue\n",
    "        else:\n",
    "            if cell_sum[target_index] < 10:\n",
    "                continue\n",
    "        epochs = []\n",
    "        for fold in range(n_kfold):\n",
    "            val_labels, best_val_out = DeepDSC(\n",
    "                res.values, null_mask.values, dim, target_index, seed\n",
    "            )\n",
    "\n",
    "        true_data_s = pd.concat(\n",
    "            [true_data_s, pd.DataFrame(val_labels.cpu().numpy())], axis=1\n",
    "        )\n",
    "        predict_data_s = pd.concat(\n",
    "            [predict_data_s, pd.DataFrame(best_val_out.cpu().numpy())], axis=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee6d72-dadf-4ce2-b793-8c8503ddf36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8870d-256e-43e1-b018-ea06be5dbdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9284d50-829f-4e83-b71f-c7c23e7acf95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcedcce-527c-4356-a2d6-06cc9588dbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genex",
   "language": "python",
   "name": "genex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
